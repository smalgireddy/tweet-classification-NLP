{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from scikitplot import plotters as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from string import punctuation\n",
    "from preprocess_twitter import tokenize as tokenizer_g\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from preprocess_twitter import tokenize as tokenizer_g\n",
    "#skplt.plot_confusion_matrix(y, preds)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input, LSTM\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "import numpy as np\n",
    "import pdb\n",
    "from nltk import tokenize\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import np_utils\n",
    "from string import punctuation\n",
    "import codecs\n",
    "import operator\n",
    "import gensim, sklearn\n",
    "from collections import defaultdict\n",
    "from batch_gen import batch_gen\n",
    "import sys\n",
    "from string import punctuation\n",
    "from preprocess_twitter import tokenize as tokenizer_g\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk import tokenize as tokenize_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#json reading using pandas\n",
    "tweets_pd = pd.read_json('classified_tweets_latest.json', orient='columns')\n",
    "tweets_pd.head()\n",
    "tweets_text = (tweets_pd['tweet'].apply(lambda tweet : eval(tweet))).apply(lambda tweet : tweet['text'])\n",
    "tweets_pd = tweets_pd.assign(tweets_text = tweets_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borne</th>\n",
       "      <th>class</th>\n",
       "      <th>conf</th>\n",
       "      <th>count</th>\n",
       "      <th>exercise</th>\n",
       "      <th>food</th>\n",
       "      <th>label_data</th>\n",
       "      <th>runid</th>\n",
       "      <th>stamp</th>\n",
       "      <th>status</th>\n",
       "      <th>statusvec</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "      <th>tweets_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Junk</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-26 06:37:29</td>\n",
       "      <td>@NicholasUnder @machineiv  dip them alive in m...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-06-05 22:49:46.662565</td>\n",
       "      <td>{'retweet_count': 0, 'favorited': False, 'crea...</td>\n",
       "      <td>824506529873350656</td>\n",
       "      <td>jason_a</td>\n",
       "      <td>@NicholasUnder @machineiv  dip them alive in m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-26 06:37:30</td>\n",
       "      <td>Looking forward to the weekend so i can eat my...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-06-05 22:49:47.360769</td>\n",
       "      <td>{'retweet_count': 0, 'favorited': False, 'crea...</td>\n",
       "      <td>824506531941117952</td>\n",
       "      <td>jason_a</td>\n",
       "      <td>Looking forward to the weekend so i can eat my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Junk</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-26 06:37:30</td>\n",
       "      <td>Baking soda that is</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-06-05 22:50:17.088828</td>\n",
       "      <td>{'retweet_count': 0, 'favorited': False, 'crea...</td>\n",
       "      <td>824506533295775744</td>\n",
       "      <td>jason_a</td>\n",
       "      <td>Baking soda that is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-26 06:37:30</td>\n",
       "      <td>I need some freakin Oreos</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-06-05 22:50:17.759452</td>\n",
       "      <td>{'retweet_count': 0, 'favorited': False, 'crea...</td>\n",
       "      <td>824506534029778944</td>\n",
       "      <td>jason_a</td>\n",
       "      <td>I need some freakin Oreos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Junk</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-26 06:37:31</td>\n",
       "      <td>Set of 10 Furniture Pumpkin Knob  https://t.co...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-06-05 22:50:18.382275</td>\n",
       "      <td>{'retweet_count': 0, 'extended_entities': {'me...</td>\n",
       "      <td>824506535233593344</td>\n",
       "      <td>jason_a</td>\n",
       "      <td>Set of 10 Furniture Pumpkin Knob  https://t.co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  borne class  conf  count exercise food label_data  runid  \\\n",
       "0  None  FOOD     1      1     None    1       Junk      1   \n",
       "1  None  FOOD     1      1     None    1  Unhealthy      1   \n",
       "2  None  FOOD     1      1     None    1       Junk      1   \n",
       "3  None  FOOD     1      1     None    1  Unhealthy      1   \n",
       "4  None  FOOD     1      1     None    1       Junk      1   \n",
       "\n",
       "                 stamp                                             status  \\\n",
       "0  2017-01-26 06:37:29  @NicholasUnder @machineiv  dip them alive in m...   \n",
       "1  2017-01-26 06:37:30  Looking forward to the weekend so i can eat my...   \n",
       "2  2017-01-26 06:37:30                                Baking soda that is   \n",
       "3  2017-01-26 06:37:30                          I need some freakin Oreos   \n",
       "4  2017-01-26 06:37:31  Set of 10 Furniture Pumpkin Knob  https://t.co...   \n",
       "\n",
       "  statusvec                        time  \\\n",
       "0      None  2017-06-05 22:49:46.662565   \n",
       "1      None  2017-06-05 22:49:47.360769   \n",
       "2      None  2017-06-05 22:50:17.088828   \n",
       "3      None  2017-06-05 22:50:17.759452   \n",
       "4      None  2017-06-05 22:50:18.382275   \n",
       "\n",
       "                                               tweet             tweetid  \\\n",
       "0  {'retweet_count': 0, 'favorited': False, 'crea...  824506529873350656   \n",
       "1  {'retweet_count': 0, 'favorited': False, 'crea...  824506531941117952   \n",
       "2  {'retweet_count': 0, 'favorited': False, 'crea...  824506533295775744   \n",
       "3  {'retweet_count': 0, 'favorited': False, 'crea...  824506534029778944   \n",
       "4  {'retweet_count': 0, 'extended_entities': {'me...  824506535233593344   \n",
       "\n",
       "    userid                                        tweets_text  \n",
       "0  jason_a  @NicholasUnder @machineiv  dip them alive in m...  \n",
       "1  jason_a  Looking forward to the weekend so i can eat my...  \n",
       "2  jason_a                                Baking soda that is  \n",
       "3  jason_a                          I need some freakin Oreos  \n",
       "4  jason_a  Set of 10 Furniture Pumpkin Knob  https://t.co...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7294, 16)\n"
     ]
    }
   ],
   "source": [
    "#tweets_pd.label_data.value_counts()\n",
    "#missing values checking for label_data\n",
    "tweets_pd = tweets_pd.loc[tweets_pd['food'] != 'None']\n",
    "tweets_pd = tweets_pd.loc[tweets_pd['label_data'] != '']\n",
    "print(tweets_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_pd['label_data'] = tweets_pd.label_data.str.lower()\n",
    "tweets_pd['tweets_text'] = tweets_pd.tweets_text.apply(lambda x: ' '.join(re.sub(r\"([!@#$]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",'',x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#after removing special characters, we left with plain data with alphanumeric characters\n",
    "#seperate the data based on their class\n",
    "tweets_pd_food = tweets_pd.loc[tweets_pd.food == '1']\n",
    "tweets_pd_borne = tweets_pd.loc[tweets_pd.borne == '1']\n",
    "tweets_pd_exercies = tweets_pd.loc[tweets_pd.exercise == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets with food dataframe shape is (7294, 16)\n",
      "tweets with borne dataframe shape is (470, 16)\n",
      "tweets with exercises dataframe shape is (2201, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"tweets with food dataframe shape is {}\".format(tweets_pd_food.shape))\n",
    "print(\"tweets with borne dataframe shape is {}\".format(tweets_pd_borne.shape))\n",
    "print(\"tweets with exercises dataframe shape is {}\".format(tweets_pd_exercies.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_792017 = pd.read_json('07292017.json', orient='columns')\n",
    "pd_8022017 = pd.read_json('08022017.json', orient='columns')\n",
    "pd_8072017 = pd.read_json('08072017.json', orient='columns')\n",
    "pd_8082017 = pd.read_json('08082017.json', orient='columns')\n",
    "pd_8242017 = pd.read_json('08242017.json', orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data = pd.concat([pd_792017, pd_8022017, pd_8072017, pd_8082017, pd_8242017], ignore_index=True)\n",
    "tweets_data_text = (tweets_data.tweet.apply(lambda tweet: eval(tweet))).apply(lambda text: text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data with 9 columns\n",
    "tweets_data = tweets_data.assign(tweets_data_text = tweets_data_text)\n",
    "#tweets_data_text.head()\n",
    "#change the labels into category codes\n",
    "tweets_data.food = tweets_data.food.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28246, 10)\n",
      "data with labels 26922\n",
      "removing data without labels\n",
      "after removing the shape of the data is(26922, 10)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_data.shape)\n",
    "#remove the tweets without label_data\n",
    "print(\"data with labels {}\".format(sum(tweets_data['label_data'] != '')))\n",
    "print(\"removing data without labels\")\n",
    "#missing values checking for label_data\n",
    "tweets_data = tweets_data.loc[tweets_data['label_data'] != '']\n",
    "print(\"after removing the shape of the data is{}\".format(tweets_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    RT @_skull_queen_: So..... has anyone ever act...\n",
      "1    RT @_skull_queen_: So..... has anyone ever act...\n",
      "2    RT @_skull_queen_: So..... has anyone ever act...\n",
      "3    RT @_skull_queen_: So..... has anyone ever act...\n",
      "4    RT @_skull_queen_: So..... has anyone ever act...\n",
      "Name: tweets_data_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_data['label_data'] = tweets_data.label_data.str.lower()\n",
    "tweets_data['tweets_text'] = tweets_data.tweets_data_text.apply(lambda x: ' '.join(re.sub(r\"([!@#$]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",'',x).split()))\n",
    "print(tweets_data.tweets_data_text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#after removing special characters, we left with plain data with alphanumeric characters\n",
    "#seperate the data based on their class\n",
    "tweets_data_food = tweets_data.loc[tweets_data.food == 1]\n",
    "tweets_data_borne = tweets_data.loc[tweets_data.borne == 1]\n",
    "tweets_data_exercies = tweets_data.loc[tweets_data.exercise == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data_food = tweets_data_food[['label_data','tweets_text']]\n",
    "tweets_data_borne = tweets_data_borne[['label_data','tweets_text']]\n",
    "tweets_data_exercies = tweets_data_exercies[['label_data','tweets_text']]\n",
    "tweets_pd_food = tweets_pd_food[['label_data','tweets_text']]\n",
    "tweets_pd_borne = tweets_pd_borne[['label_data','tweets_text']]\n",
    "tweets_pd_exercies = tweets_pd_exercies[['label_data','tweets_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_food = pd.concat([tweets_data_food, tweets_pd_food])\n",
    "tweets_borne = pd.concat([tweets_data_borne, tweets_pd_borne])\n",
    "tweets_exercies = pd.concat([tweets_data_exercies, tweets_pd_exercies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets with food dataframe shape is (19504, 2)\n",
      "tweets with borne dataframe shape is (16550, 2)\n",
      "tweets with exercises dataframe shape is (2469, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"tweets with food dataframe shape is {}\".format(tweets_food.shape))\n",
    "print(\"tweets with borne dataframe shape is {}\".format(tweets_borne.shape))\n",
    "print(\"tweets with exercises dataframe shape is {}\".format(tweets_exercies.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#'healthy','unhealthy','junk','relevant','irrelevant'\n",
    "food_labels = ['healthy','unhealthy','junk']\n",
    "borne_labels = exercise_labels = ['junk','relevant','irrelevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_food = tweets_food.loc[tweets_food.label_data.isin(food_labels)]\n",
    "tweets_borne = tweets_borne.loc[tweets_borne.label_data.isin(borne_labels)]\n",
    "tweets_exercies = tweets_exercies.loc[tweets_exercies.label_data.isin(exercise_labels)]\n",
    "\n",
    "#tweets_data_food.label_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glove_tokenize(text):\n",
    "    text = tokenizer_g(text)\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOPWORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label_data', 'tweets_text'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_food.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NicholasUnder machineiv dip them alive in melted chocolate and encase em Cover them in seeds and peanut butter and let birds eat em 25\n"
     ]
    }
   ],
   "source": [
    "print(tweets_food.tweets_text[0])\n",
    "#glove_tokenize(tweets_pd.tweets_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec_model = gensim.models.Word2Vec.load_word2vec_format('glove.twitter.27B.25d.txt')\n",
    "embeddings_index = {}\n",
    "f = open('glove.twitter.27B.25d.txt')\n",
    "for line in f:\n",
    "    #print(line)\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhoshm/.local/lib/python3.5/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(tweets_food.tweets_text)\n",
    "sequences = tokenizer.texts_to_sequences(tweets_food.tweets_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = max(map(lambda x:len(x), tweets_food.tweets_text))\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18230 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17894, 143)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18230 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(256, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(512, 5, padding=\"same\", activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35, padding=\"same\")(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "pred = Dense(3, activation='softmax', name = 'fc')(l_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ot = K.concatenate([l_dense, preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(sequence_input, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VALIDATION_SPLIT = 0.2\n",
    "# labels = np.asarray(tweets_food.label_data)\n",
    "# num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "# x_train = data[:-num_validation_samples]\n",
    "# y_train = labels[:-num_validation_samples]\n",
    "# x_val = data[-num_validation_samples:]\n",
    "# y_val = labels[-num_validation_samples:]\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(tweets_food['label_data'])\n",
    "#print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "labels = onehot_encoder.fit_transform(integer_encoded)\n",
    "#print(tweets_food['label_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236    unhealthy\n",
       "237    unhealthy\n",
       "242      healthy\n",
       "243      healthy\n",
       "259    unhealthy\n",
       "Name: label_data, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_food['label_data'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "healthy      8316\n",
       "unhealthy    6448\n",
       "junk         3130\n",
       "Name: label_data, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_food.label_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhoshm/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(data, labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, val_x, y_train, val_y = train_test_split(train_x, train_y, test_size = 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "11452/11452 [==============================] - 14s 1ms/step - loss: 1.0049 - acc: 0.4986\n",
      "Epoch 2/2\n",
      "11452/11452 [==============================] - 12s 1ms/step - loss: 0.9624 - acc: 0.5260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83643e0b00>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 2, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['healthy', 'junk', 'unhealthy'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "tweets_food['label_data'] = encoder.fit_transform(tweets_food['label_data'])\n",
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder.test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#word2vec genism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize text into array of words\n",
    "def tokenizeData(pdSeries):\n",
    "    #corpus = pdSeries.apply(lambda x: x.replace(\"\\n\",\"\").split())\n",
    "    corpus = [z.lower().replace('\\n','').split() for z in pdSeries]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_text = tokenizeData(tweets_exercies.tweets_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train word2vec on each tweet text\n",
    "n_dim = 250\n",
    "#Initialize model and build vocab\n",
    "tweet2vec = Word2Vec(size=n_dim, min_count=5)\n",
    "tweet2vec.build_vocab(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114931"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet2vec.train(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweet2vec.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build word vector for training set by using the average value of all word vectors in the tweet, then scale\n",
    "def buildWord2Vec(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += tweet2vec[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs = np.concatenate([buildWord2Vec(z, n_dim) for z in tweet_text])\n",
    "train_vecs = scale(train_vecs)\n",
    "\n",
    "#Train word2vec on test tweets\n",
    "#imdb_w2v.train(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "y = lb.fit_transform(tweets_exercies.label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(train_vecs, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.49\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "maxiter = 1000\n",
    "batch = 150\n",
    "nnet = MLPClassifier(alpha = 1e-8, solver = 'adam', batch_size=batch)\n",
    "nnet.fit(train_x, train_y)\n",
    "\n",
    "print(\"Test Accuracy: {:.2f}\".format(accuracy_score(nnet.predict(test_x), test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of RF is 0.6013631688384189\n",
      "precision score of RF is 0.6184096771718968\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "# Initialize a Random Forest classifier with 500 trees\n",
    "exercises_forest = RandomForestClassifier(n_estimators = 500, criterion='entropy', n_jobs = -1, random_state = 1) \n",
    "exercises_forest.fit(train_x, train_y)\n",
    "pred = exercises_forest.predict(test_x)\n",
    "print('f1 score of RF is {}'.format(f1_score(test_y, pred, average='weighted')))\n",
    "print('precision score of RF is {}'.format(precision_score(test_y, pred, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model is 0.5938775510204082\n",
      "f1 score of SVM is 0.5868880660675946\n",
      "precision score of SVM is 0.5882995745591738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "exercises_svm = SVC(kernel='rbf', C=10, gamma = 1)\n",
    "exercises_svm.fit(train_x, train_y)\n",
    "pred = exercises_svm.predict(test_x)\n",
    "print(\"accuracy of the model is {}\".format(exercises_svm.score(test_x, test_y)))\n",
    "print('f1 score of SVM is {}'.format(f1_score(test_y, pred, average='weighted')))\n",
    "print('precision score of SVM is {}'.format(precision_score(test_y, pred, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doc2vec\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec simply converts a word into a vector.\n",
    "\n",
    "Doc2Vec not only does that, but also aggregates all the words in a sentence into a vector. To do that, it simply treats a sentence label as a special word, and does some voodoo on that special word. Hence, that special word is a label for a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in enumerate(tweets):\n",
    "        label = \"{}_{}\".format(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "food_labels = lb.fit_transform(tweets_food.label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(tweets_food.tweets_text, food_labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize text into array of words\n",
    "def tokenizeData( tokens_only, pdSeries,lab):\n",
    "    #corpus = pdSeries.apply(lambda x: x.replace(\"\\n\",\"\").split())\n",
    "    #corpus = [np.array(z.lower().replace('\\n','').split()) for z in pdSeries]\n",
    "    for i, line in enumerate(pdSeries):\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "        else:\n",
    "             # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [\"\".join([lab,str(i)])])\n",
    "    #return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# def read_corpus(fname, tokens_only=False):\n",
    "#     with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "#         for i, line in enumerate(f):\n",
    "#             if tokens_only:\n",
    "#                 yield gensim.utils.simple_preprocess(line)\n",
    "#             else:\n",
    "#                 # For training data, add tags\n",
    "#                 yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_food_text_train = list(tokenizeData(False, train_x, \"train\"))\n",
    "tweet_food_text_test = list(tokenizeData(False, test_x, \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweet_food_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14315,)\n",
      "(14315,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train = labelizeTweets(train_x, 'TRAIN')\n",
    "# x_test = labelizeTweets(test_x, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate our two Doc2Vec models, DM and DBOW. The gensim documentation suggests training over the data multiple times and either adjusting the learning rate or randomizing the order of input at each pass. We then collect the movie review vectors learned by the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#instantiate our DM and DBOW models\n",
    "size = 300\n",
    "model_dm = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, workers=3)\n",
    "#DBOW\n",
    "model_dbow = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, dm=0, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_food_text_model = [item for pair in zip(tweet_food_text_train, tweet_food_text_test) for item in pair]\n",
    "#to add list just use + operatr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build vocab over all tweets\n",
    "model_dm.build_vocab(tweet_food_text_train+tweet_food_text_test)\n",
    "model_dbow.build_vocab(tweet_food_text_train+tweet_food_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14315,)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_dm.train(tweet_food_text_train, total_examples=model_dm.corpus_count, epochs=model_dm.iter)\n",
    "#model_dbow.train(tweet_food_text_train, total_examples=model_dbow.corpus_count, epochs=model_dbow.iter)\n",
    "perm = np.random.permutation(train_x.shape[0], )\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We pass through the data set multiple times, shuffling the training reviews each time to improve accuracy.\n",
    "all_train_tweets = tweet_food_text_train\n",
    "for epoch in range(10):\n",
    "      \n",
    "    model_dm.alpha -= 0.002\n",
    "    model_dbow.alpha -= 0.002\n",
    "   #model.min_alpha = model.alpha\n",
    "    model_dm.train(all_train_tweets)\n",
    "    model_dbow.train(all_train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model_dm.alpha -= 0.002\n",
    "    model_dbow.alpha -= 0.002\n",
    "   #model.min_alpha = model.alpha\n",
    "    model_dm.train(tweet_food_text_test)\n",
    "    model_dbow.train(tweet_food_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get training set vectors from our models\n",
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model[z[0]]) for z in corpus]\n",
    "    return np.concatenate(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_vecs_dm = getVecs(model_dm, tweet_food_text_train, size)\n",
    "#np.array(model_dm[np.asarray(tweet_food_text_train[0])[0]])\n",
    "#tweet_food_text_train[0][0]\n",
    "#train_vecs_dm.shape\n",
    "#model_dm.most_similar(\"money\")\n",
    "len(model_dm.docvecs[\"test0\"])\n",
    "train_vecs_dm = [model_dm.docvecs[\"\".join([\"train\",str(i)])] for i, line in enumerate(train_x)]\n",
    "test_vecs_dm = [model_dm.docvecs[\"\".join([\"test\",str(i)])] for i, line in enumerate(test_x)]\n",
    "train_vecs_dbow = [model_dbow.docvecs[\"\".join([\"train\",str(i)])] for i, line in enumerate(train_x)]\n",
    "test_vecs_dbow = [model_dbow.docvecs[\"\".join([\"test\",str(i)])] for i, line in enumerate(test_x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3579"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_vecs_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs = np.hstack((train_vecs_dm, train_vecs_dbow))\n",
    "test_vecs = np.hstack((test_vecs_dm, test_vecs_dbow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of RF is 0.33922807981954034\n",
      "precision score of RF is 0.3094953743463355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhoshm/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/santhoshm/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "# Initialize a Random Forest classifier with 500 trees\n",
    "food_forest = RandomForestClassifier(n_estimators = 500, criterion='entropy', n_jobs = -1, random_state = 1) \n",
    "food_forest.fit(train_vecs, train_y)\n",
    "pred = food_forest.predict(test_vecs)\n",
    "print('f1 score of RF is {}'.format(f1_score(test_y, pred, average='weighted')))\n",
    "print('precision score of RF is {}'.format(precision_score(test_y, pred, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.35\n"
     ]
    }
   ],
   "source": [
    "maxiter = 1000\n",
    "batch = 150\n",
    "nnet = MLPClassifier(alpha = 1e-8, solver = 'adam', batch_size=batch)\n",
    "nnet.fit(train_vecs, train_y)\n",
    "\n",
    "print(\"Test Accuracy: {:.2f}\".format(accuracy_score(nnet.predict(test_vecs), test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fishs', 0.23842597007751465),\n",
       " ('kinda', 0.2048811912536621),\n",
       " ('urge', 0.20292916893959045),\n",
       " ('cody', 0.20092925429344177),\n",
       " ('vital', 0.20052559673786163),\n",
       " ('lfo', 0.1966307908296585),\n",
       " ('memeiorde', 0.19456884264945984),\n",
       " ('drugs', 0.19377486407756805),\n",
       " ('danyell', 0.19292503595352173),\n",
       " ('ready', 0.1897403597831726)]"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.similar_by_word(\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
